apiVersion: batch/v1
kind: Job
metadata:
  name: achu-job-1
  namespace: wenglab-interpretable-ai
spec:
  template:
    spec:
      containers:
      - name: worker
        image: pytorch/pytorch:2.2.0-cuda12.1-cudnn8-runtime
        workingDir: /am-oth-vol/
        tty: true
        command: ["/bin/bash", "-c"]
        args:
          - |
            cd /am-oth-vol/
            pip install transformers datasets scikit-learn tqdm accelerate pyyaml -q
            python scripts/collect_traces.py \
              --model deepseek-ai/DeepSeek-R1-Distill-Llama-8B \
              --output_dir /am-oth-vol/outputs/traces \
              --n_samples 500
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token-secret
              key: token
        resources:
          limits:
            memory: 48Gi
            cpu: "4"
            nvidia.com/gpu: "1"
          requests:
            memory: 48Gi
            cpu: "4"
            nvidia.com/gpu: "1"
        volumeMounts:
        - name: am-oth-vol
          mountPath: /am-oth-vol
        - name: dshm
          mountPath: /dev/shm
      volumes:
      - name: am-oth-vol
        persistentVolumeClaim:
          claimName: am-oth-vol
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 16Gi
      restartPolicy: Never
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - Tesla-V100-SXM2-32GB
